# Deploy
# Deployment with LitServe 
- optimized for AI serving (add custom code also)
- full flexibility

```sh
pip install litserve
```

torch community:: 
- TorchServe(java)

python community: fastAPI


Optimization comparison as meant *number of request it can handle*

litserver ( for server https:localhost:8080 )
- decoder_req
- predict-fn
- encode-response {response_code:200, value}


once server starts "https://localhost:8080"
- receives as a *dict* from a client /byte/
- get values by base64
- get proper value and pass to model 

client_machine -> send-img encode-with-base4 -> byte/travels-internet -> receive by model -> base64-decode -> model(x) -> encode-res*basee64* -> send back
 
https://saturncloud.io/blog/how-to-install-pytorch-on-the-gpu-with-docker/

setup_fn will call 4 times in gpu


predit_fn should return probablities and value:: **not ternsor object**
encode-res-fn wrap predict_result and wrape base64 so that client can understand



```Dockerfile
FROM nvidia/cuda:11.2.0-runtime-ubuntu20.04

# install utilities
RUN apt-get update && \
    apt-get install --no-install-recommends -y curl

ENV CONDA_AUTO_UPDATE_CONDA=false \
    PATH=/opt/miniconda/bin:$PATH
RUN curl -sLo ~/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh \
    && chmod +x ~/miniconda.sh \
    && ~/miniconda.sh -b -p /opt/miniconda \
    && rm ~/miniconda.sh \
    && sed -i "$ a PATH=/opt/miniconda/bin:\$PATH" /etc/environment

# Installing python dependencies
RUN python3 -m pip --no-cache-dir install --upgrade pip && \
    python3 --version && \
    pip3 --version

RUN pip3 --timeout=300 --no-cache-dir install torch==1.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html

COPY ./requirements.txt .
RUN pip3 --timeout=300 --no-cache-dir install -r requirements.txt

# Copy model files
COPY ./model /model

# Copy app files
COPY ./app /app
WORKDIR /app/
bash
```

inboun:  anthying that comes ec2 that's inbound  `tensorboard:6006`
outbound: ec2 can access everything



clinet.py => sending request to server which serving model
- before sending encode into PIL


in litserver
- we have *decode_req_fn* which calls first which  deencodes  bytes to PIL

make sure it handles multiple request 
status_code:
- 200 success
- 500 bad happens on server side

**benchmarking**

## Batching
if you've multiple req coming server wait 0.0001 if comes and batch-size statisfy it'll do `model(x)` 
client need not do at client side


- trade-off between batching and latency

use **T4 g4dn instance** 16 GB RAM **T4**

`note`:
if request  hitting is sparse then use `LAMDA` service instead of EC2

if request not hitten power of 2 then, add dummy data pass to model(x)


**What is throughoutput** 

#### Step todo in batching
create another function called **batch** grap all inputs and batch and pass to model(x)  *batch_timeout*
- image decode
- torch.stach(images)
- model(x)
- after model unbatch 
- each output wrap encode_response for every unbatch

**base64** and **transforms** and **decode** and **to(device)** are CPU heavy

PIL --> PILS use concorrancy

number of threads = number of workers
best we go for 2* numberof-threads



### context switching
-4 core
- so many procesors are running
so context switching
current running program is context


##  
concuranncy=32 so 32 threads comming in
keep num-of-thread=num-of-cores





**parallel DECODING** and check how muc hcontext switching is happening
- workers-per-device
image = 3,224,224,
batch=64
pixels = 9633792
96337928*32=308281344   bites=32 for float
308281344/8 => byte 38535168.0
38535168.0/1024 -> KB
38535168.0/1024/1024 -> MB  = 36.75MB for single-batch										

**Mixed Precision**

**PQDM** ~tqdm but for palllel-jobs


#pragma omp parallel 
- makes life easier for runnning code block in multi-thread without changing 




## Dockerfile
https://github.com/anibali/docker-pytorch



# LLM DEPLOY OpenAI spec
70B Nemotron = 130GB Model on 16-bit float
SMLOv2 model






# DEVOPS

```bash
%% files were edited in the last 10 mins
find . -type f -mmin -10 

%% Check the disk usage to find out which files are taking the most disk space:
sort du -hsx . | sort -rh | head - n 5

%% run commands one by one
comand1 ; command2; command3

%% run commands one by one if one fails stop
comand1 && command2 && command3


ls -laSh

%% Get the virtual memory statistics report:
vmstat 1 5 
- every 1 sec re-measure and report every sec
- report 5 times and stop

%% get detailed permission
getfacl path_to_file.wtf

%% export History with Time-Stamp
export HISTTIMEFORMAT="%d/%m/%y %T "   


%% run last command with sudo permission
sudo !!


date
date +"%H-%M-%S::%T"


%% ports 
netstat -tulanp

%% free
free -mlth


%% top process eating memory
ps auxf | sort -nr -k 4 | head -10

%% get top process eating cpu
ps auxf | sort -nr -k 3 | head -10


%% cpu info
lscpu


%% disk free
df -H 

%% disk usage
du -ch

?? nohup 
?? ifconfig
?? curl

ip addr
scp 
rsync
bg
fg
jobs

```






    
